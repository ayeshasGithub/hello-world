{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature _Engineering_start_with.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "17S9WNv3wJ0thKeX1Pwe76JDjBQJI5UvN",
      "authorship_tag": "ABX9TyNgtM2JlzEFZ08t9s5I9hqC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayeshasGithub/hello-world/blob/master/Feature__Engineering_start_with.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQyXSxMVJkT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Data/LMK_WoolSales_SaleSummary.csv', encoding= 'unicode_escape')\n",
        "df_big=df\n",
        "df_small=df.sample(frac=0.01, random_state=1)#subset of df\n",
        "#fig = X_train.groupby(['Cabin_ordered'])['Survived'].mean().plot() #group by\n",
        "#df.shape #how many rows and columns\n",
        "#df['Brand'].head(5)\n",
        "#df.columns #see column names\n",
        "#df.isna().sum() #which column has how many null values\n",
        "\n",
        "#remove 3600 rows based on sold outcome\n",
        "df_sold = df[df['SaleOutcome'] == 0] \n",
        "\n",
        "'''\n",
        "#df_sold-df_test using sklearn easy technique\n",
        "from sklearn.model_selection import train_test_split\n",
        "df_sold_train, df_sold_test = train_test_split(df_sold, test_size=0.00001)\n",
        "df_all_train, df_all_test = train_test_split(df, test_size=0.00001)\n",
        "\n",
        "print(df_sold_train.shape)\n",
        "print(df_sold_test.shape)\n",
        "'''\n",
        "print(df_sold.head(5))\n",
        "\n",
        "!pip install pickle-mixin\n",
        "import pickle\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMOLmiVUca4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_year_month_day(df_date,what):\n",
        "  import datetime\n",
        "  df_date = df_date.astype(\"datetime64\")#convert type to datetime64\n",
        "  if what=='day':\n",
        "    df_date['Day']=df_date['SaleDate'].dt.day#extract month\n",
        "  if what=='month':\n",
        "    df_date['Month']=df_date['SaleDate'].dt.month#extract month\n",
        "  if what=='year':\n",
        "    df_date['Year']=df_date['SaleDate'].dt.year#extract year\n",
        "\n",
        "  return df_date\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "def see_correlation(df_test,column_names,target_name,df_t):#df_test=> dataframe with column and target\n",
        "  import numpy\n",
        "  #df_test[target_name]=df[[target_name]]#add target column\n",
        "  print('helo see_correlation')\n",
        "  for col in column_names:\n",
        "    encoded_feature = df_test[col].values\n",
        "    #first_correlation = numpy.corrcoef(df_test[target_name].values, encoded_feature)[0][1]\n",
        "    first_correlation = numpy.corrcoef(df_t[target_name].values, encoded_feature)[0][1]\n",
        "    print(col,' correlation ', first_correlation)\n",
        "        \n",
        "\n",
        "#normalization function\n",
        "def scaleColumns(df_test, cols_to_scale):\n",
        "    #import libraries\n",
        "    import pandas as pd\n",
        "    from sklearn import preprocessing\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    print('cols to scale',cols_to_scale)\n",
        "    for col in cols_to_scale:\n",
        "      df_test[col] = pd.DataFrame(min_max_scaler.fit_transform(pd.DataFrame(df_test[col])),columns=[col])\n",
        "    return df_test\n",
        "\n",
        "\n",
        "\n",
        "#standardization function\n",
        "def standardColumns(df, cols_to_scale):\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    import pandas as pd\n",
        "    scaler=StandardScaler()\n",
        "    \n",
        "    for col in cols_to_scale:\n",
        "        df[col] = pd.DataFrame(scaler.fit_transform(pd.DataFrame(df[col])),columns=[col])\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def label_encoder_date(df_test,columns):\n",
        "  from sklearn import preprocessing\n",
        "  le = preprocessing.LabelEncoder()\n",
        "  \n",
        "  for col in columns:\n",
        "    encoded=le.fit_transform(df_test[col].astype(str))\n",
        "    df_test[col] = pd.DataFrame({col: encoded})\n",
        "  #'''  \n",
        "  return df_test\n",
        "\n",
        "\n",
        "\n",
        "# https://necromuralist.github.io/kaggle-competitions/posts/mean-encoding-the-competition-data/\n",
        "def meanEncoding(df_c, cols_to_scale,target,df_t):#df_t==> column with target values\n",
        "    import numpy \n",
        "    df_c[target]=df_t[target]   #add target column to df_c\n",
        "    for col in cols_to_scale:\n",
        "        print(col,end=' ')\n",
        "        item_id_target_mean = df_c.groupby([col])[target].mean()\n",
        "        #item_id_target_mean = df_c.groupby([col])[target].transform('mean')\n",
        "        df_c[col] = df_c[col].map(item_id_target_mean)\n",
        "        encoded_feature = df_c[col].values\n",
        "        first_correlation = numpy.corrcoef(df_c[target].values, encoded_feature)[0][1]\n",
        "        #first_correlation = numpy.corrcoef(df_t[target].values, encoded_feature)[0][1]\n",
        "        print(first_correlation)\n",
        "    df_c=df_c.loc[:,cols_to_scale]    \n",
        "    return df_c\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# https://necromuralist.github.io/kaggle-competitions/posts/mean-encoding-the-competition-data/\n",
        "def kfoldSchemeEncoding(df_c, cols_to_scale,target):\n",
        "      \n",
        "    folder = KFold(n_splits=5, shuffle=False)\n",
        "    train_new = pd.DataFrame(index=df_c.index, columns=df_c.columns) \n",
        "    \n",
        "    for col in cols_to_scale:\n",
        "    \n",
        "        column = col#\"item_id\"\n",
        "        encoded_column = column + \"_mean_target\"\n",
        "        \n",
        "           #all_data=df_c\n",
        "        train_new[encoded_column] = numpy.nan\n",
        "        \n",
        "\n",
        "\n",
        "        for training_index, validation_index in folder.split(df_c):\n",
        "\n",
        "          #print('index',training_index,validation_index)\n",
        "          #print(encoded_column)\n",
        "\n",
        "\n",
        "          x_train = df_c.iloc[training_index].copy()\n",
        "          x_validation = df_c.iloc[validation_index].copy()\n",
        "          means = x_validation[column].map(x_train.groupby(column)[target].mean())\n",
        "          x_validation[encoded_column] = means\n",
        "          # train_new is a dataframe copy we made of the training data\n",
        "          train_new.iloc[validation_index] = x_validation\n",
        "\n",
        "        \n",
        "        #train_new.fillna(NAN_VALUE, inplace=True)\n",
        "        encoded_feature = train_new[encoded_column].values#.SaleNbrSellingCntr_mean_target.values#item_id_mean_target.values\n",
        "        corr = numpy.corrcoef(df_c[target].values, encoded_feature)[0][1]\n",
        "        print(corr)\n",
        "        #grader.submit_tag('KFold_scheme', corr)\n",
        "        #'''\n",
        "def check_for_nulls(df_test):\n",
        "  print(df_test.isna().sum())"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbN3TTzKsj3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_label_encoder_date(df_test, columns,purpose='train'):#it needs to pass in the train data (df_tr) also during making test set since we are not saving anything\n",
        "  from sklearn import preprocessing\n",
        "  le = preprocessing.LabelEncoder()\n",
        "\n",
        "  if purpose=='train':\n",
        "\n",
        "    #save training data\n",
        "    df_tr=df_test\n",
        "    with open('train_data_date.pickle', 'wb') as f:\n",
        "      pickle.dump(df_tr, f)\n",
        "    \n",
        "    for col in columns:\n",
        "      encoded=le.fit_transform(df_test[col].astype(str))\n",
        "      df_test[col] = pd.DataFrame({col: encoded})\n",
        "        \n",
        "    return df_test\n",
        "\n",
        "  else:\n",
        "    print('label on test data')\n",
        "    #load df_tr\n",
        "    with open('train_data_date.pickle', 'rb') as f:\n",
        "      df_tr = pickle.load(f)\n",
        "    print(df_tr.columns)\n",
        "    print(df_test.columns)\n",
        "    for col in columns:\n",
        "      le.fit(df_tr[col].astype(str))\n",
        "      encoded=le.transform(df_test[col].astype(str))\n",
        "      df_test[col] = pd.DataFrame({col: encoded})\n",
        "\n",
        "    return df_test"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQXydw_A---S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://stackoverflow.com/questions/21057621/sklearn-labelencoder-with-never-seen-before-values\n",
        "#fit(self, y)-->Fit label encoder, le.fit(X)\n",
        "#fit_transform(self, y)--> Fit label encoder and return encoded labels\n",
        "\n",
        "def apply_label_encoder(df_test, columns,purpose='train'):#it needs to pass in the train data (df_tr) also during making test set since we are not saving anything\n",
        "  from sklearn import preprocessing\n",
        "  le = preprocessing.LabelEncoder()\n",
        "\n",
        "  if purpose=='train':\n",
        "\n",
        "    #save training data\n",
        "    df_tr=df_test\n",
        "    with open('train_data.pickle', 'wb') as f:\n",
        "      pickle.dump(df_tr, f)\n",
        "    \n",
        "    for col in columns:\n",
        "      encoded=le.fit_transform(df_test[col].astype(str))\n",
        "      df_test[col] = pd.DataFrame({col: encoded})\n",
        "        \n",
        "    return df_test\n",
        "\n",
        "  else:\n",
        "    print('label on test data')\n",
        "    #load df_tr\n",
        "    with open('train_data.pickle', 'rb') as f:\n",
        "      df_tr = pickle.load(f)\n",
        "    print(df_tr.columns)\n",
        "    print(df_test.columns)\n",
        "    for col in columns:\n",
        "      le.fit(df_tr[col].astype(str))\n",
        "      encoded=le.transform(df_test[col].astype(str))\n",
        "      df_test[col] = pd.DataFrame({col: encoded})\n",
        "\n",
        "    return df_test"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDt72SpZV80X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  \n",
        "# https://necromuralist.github.io/kaggle-competitions/posts/mean-encoding-the-competition-data/\n",
        "def apply_meanEncoding(df_c, cols_to_scale,target,df_t,purpose='train'):#df_t==> column with target values\n",
        "    import numpy \n",
        "    item_id_target_mean_dic={}\n",
        "    if purpose=='train':\n",
        "      df_c[target]=df_t[target]   #add target column to df_c\n",
        "      for col in cols_to_scale:\n",
        "          print(col,end=' ')\n",
        "          item_id_target_mean = df_c.groupby([col])[target].mean()\n",
        "          item_id_target_mean_dic[col]=item_id_target_mean\n",
        "          df_c[col] = df_c[col].map(item_id_target_mean)\n",
        "          encoded_feature = df_c[col].values\n",
        "          first_correlation = numpy.corrcoef(df_c[target].values, encoded_feature)[0][1]\n",
        "          print(first_correlation)\n",
        "      #save item_id_target_mean_dic\n",
        "      with open('mean_encoded.pickle', 'wb') as f:\n",
        "        pickle.dump(item_id_target_mean_dic, f)\n",
        "      \n",
        "      df_c=df_c.loc[:,cols_to_scale] \n",
        "      \n",
        "      \n",
        "\n",
        "    else:\n",
        "        print('implement')\n",
        "        #load what \n",
        "        with open('mean_encoded.pickle', 'rb') as f:\n",
        "          item_id_target_mean_dic= pickle.load(f)\n",
        "\n",
        "        for col in cols_to_scale:\n",
        "          item_id_target_mean=item_id_target_mean_dic[col]\n",
        "          df_c[col] = df_c[col].map(item_id_target_mean)\n",
        "          \n",
        "    return df_c\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTDgo003urh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import KFold #KFold splits data set in the given number of folds\n",
        "\n",
        "# This way we have randomness and are able to reproduce the behaviour within this cell.\n",
        "np.random.seed(13)\n",
        "\n",
        "def impact_coding(data, feature, target='SalePrice'):\n",
        "    '''\n",
        "    In this implementation we get the values and the dictionary as two different steps.\n",
        "    This is just because initially we were ignoring the dictionary as a result variable.\n",
        "   \n",
        "    In this implementation the KFolds use shuffling. If you want reproducibility the cv could be moved to a parameter.\n",
        "    '''\n",
        "    n_folds = 10 # data will be splitted into 20 folds\n",
        "    n_inner_folds = 5 #data will be splitted into 10 folds\n",
        "    impact_coded = pd.Series()\n",
        "    \n",
        "    oof_default_mean = data[target].mean() # Gobal mean to use by default (you could further tune this)\n",
        "    kf = KFold(n_splits=n_folds, shuffle=True) # shuffle data before splitting into batches. Note that the samples within each split will not be shuffled.\n",
        "    oof_mean_cv = pd.DataFrame()\n",
        "    split = 0\n",
        "    county=0\n",
        "    #train_index=infold, test_index=oof\n",
        "    for infold, oof in kf.split(data[feature]):# the kf class's split, splits data into 20(given) folds and each fold is called by a loop\n",
        "\n",
        "            impact_coded_cv = pd.Series()\n",
        "            kf_inner = KFold(n_splits=n_inner_folds, shuffle=True)#for one split, kf_inner again splits it to 10 folds\n",
        "            inner_split = 0\n",
        "            inner_oof_mean_cv = pd.DataFrame()\n",
        "            oof_default_inner_mean = data.iloc[infold][target].mean()# 1st mean, not group by feature\n",
        "            county=county+1\n",
        "            print('inner',county)\n",
        "            for infold_inner, oof_inner in kf_inner.split(data.iloc[infold]):\n",
        "                # The mean to apply to the inner oof split (a 1/n_folds % based on the rest)\n",
        "                oof_mean = data.iloc[infold_inner].groupby(by=feature)[target].mean() #2nd mean, group by feature\n",
        "                \n",
        "                impact_coded_cv = impact_coded_cv.append(data.iloc[infold].apply(\n",
        "                            lambda x: oof_mean[x[feature]]\n",
        "                                      if x[feature] in oof_mean.index\n",
        "                                      else oof_default_inner_mean\n",
        "                            , axis=1))\n",
        "\n",
        "                # Also populate mapping (this has all group -> mean for all inner CV folds)\n",
        "                inner_oof_mean_cv = inner_oof_mean_cv.join(pd.DataFrame(oof_mean), rsuffix=inner_split, how='outer')\n",
        "                inner_oof_mean_cv.fillna(value=oof_default_inner_mean, inplace=True)\n",
        "                inner_split += 1\n",
        "\n",
        "            # Also populate mapping\n",
        "            oof_mean_cv = oof_mean_cv.join(pd.DataFrame(inner_oof_mean_cv), rsuffix=split, how='outer')\n",
        "            oof_mean_cv.fillna(value=oof_default_mean, inplace=True)\n",
        "            split += 1\n",
        "            \n",
        "            impact_coded = impact_coded.append(data.iloc[oof].apply(\n",
        "                            lambda x: inner_oof_mean_cv.loc[x[feature]].mean()\n",
        "                                      if x[feature] in inner_oof_mean_cv.index\n",
        "                                      else oof_default_mean\n",
        "                            , axis=1))\n",
        "\n",
        "    return impact_coded, oof_mean_cv.mean(axis=1), oof_default_mean\n",
        "\n",
        "\n",
        "\n",
        "# Apply the encoding to training and test data, and preserve the mapping\n",
        "def apply_impact_encoder(df_c, cols_to_scale,target,df_t,purpose='train'):\n",
        "  #categorical_features=features_c\n",
        "  #train_data=df_big\n",
        "  #add tareget column\n",
        "  df_c[target]=df_t\n",
        "  if purpose=='train':\n",
        "    impact_coding_map_dic = {}\n",
        "    for col in cols_to_scale:\n",
        "        print(\"Impact coding for {}\".format(col))\n",
        "        df_c[col], impact_coding_mapping, default_coding = impact_coding(df_c, col)\n",
        "        impact_coding_map_dic[col] = (impact_coding_mapping, default_coding)\n",
        "    \n",
        "    #save item_id_target_mean_dic\n",
        "    with open('impact.pickle', 'wb') as f:\n",
        "        pickle.dump(impact_coding_map_dic, f)\n",
        " \n",
        "\n",
        "  else:\n",
        "    \n",
        "    with open('impact.pickle', 'rb') as f:\n",
        "      impact_coding_map_dic = pickle.load(f)\n",
        "\n",
        "    for col in cols_to_scale:\n",
        "      \n",
        "      mapping, default_mean = impact_coding_map_dic[col]\n",
        "\n",
        "      print(col, 'here',mapping, default_mean)\n",
        "      df_c[col] = df_c.apply(lambda x: mapping[x[col]]\n",
        "                                            if x[col] in mapping \n",
        "                                            else default_mean\n",
        "                                                               , axis=1)\n",
        "  return df_c[cols_to_scale]#return only scaled ones without tagret\n",
        "    "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SovzEcHP6cAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "20276824-ddd3-40cb-f319-ad22d07fa08f"
      },
      "source": [
        "#feature set 4 (with mean encoding and without not-sold wool, not sold wool are filtered)\n",
        "def select_engg_features(df_test,purpose):#update this function based on selected feature, encoding you want to apply and filter technique\n",
        "    '''\n",
        "      * the categorical (seller) variables ==> label+impact\n",
        "    '''\n",
        "    df=df_test\n",
        "    #target variable:\n",
        "    label=['SalePrice']\n",
        "    target=['SalePrice']\n",
        "    df_target=df.loc[:,target]#create view\n",
        "\n",
        "\n",
        "    #weight variables:\n",
        "    features_w=['NetWeight']#from all the weights only using net weight since others are correlated\n",
        "    df_w=df.loc[:,features_w]#create view\n",
        "    df_w=scaleColumns(df_w,features_w)#apply normalization\n",
        "    #see_correlation(df_w,features_w,'SalePrice',df_target)\n",
        "    #check_for_nulls(df_w)\n",
        "\n",
        "\n",
        "    #date variables:\n",
        "    features_d=['SaleDate']#SaleDate will change to 'Month'\n",
        "    df_date=df.loc[:,features_d]#create view\n",
        "    df_date= extract_year_month_day(df_date,'month')#extract month column, this will be added to the dataframe as a new column 'Month'\n",
        "    df_d=apply_label_encoder_date(df_date[['Month']],['Month'],purpose)#apply label encoder: deal month as categorical or the model would give the most importance to 12\n",
        "    features_d=['Month']\n",
        "    #see_correlation(df_d,features_d,'SalePrice',df_target)\n",
        "    #check_for_nulls(df_d)\n",
        "\n",
        "    #seller variables:\n",
        "    features_c_1=['SaleNbrSellingCntr', 'Brand','StandardBaleDesc']\n",
        "    features_c_2=['Region','Division','Branch','WAM','Agent']\n",
        "    features_c=features_c_1+features_c_2\n",
        "    df_c=df.loc[:,features_c]#create view\n",
        "    df_c.fillna('unknown')#replace missing values\n",
        "    df_c=apply_label_encoder(df_c,features_c,purpose)#apply label encoder\n",
        "    #see_correlation(df_c,features_c,'SalePrice',df_target)\n",
        "    #df_c=apply_meanEncoding(df_c, features_c,'SalePrice',df_target,purpose)#apply mean encoder after label encoder, mean encoding displays correlation also\n",
        "    df_c=apply_impact_encoder(df_c, features_c,'SalePrice',df_target,purpose)\n",
        "\n",
        "    #check_for_nulls(df_c)\n",
        "\n",
        "\n",
        "\n",
        "    #quality variables: \n",
        "    features_q_1 = ['Micron','Curvature','VMB','Yield1','FolioProceeds']\n",
        "    features_q_2=['FDMean','FDCOEfficient','FDComfort']\n",
        "    features_q_3=['StapleLength','StapleLengthCV','StapleStrength','StapleStrengthLowest25']\n",
        "    features_q_4=['TheoreticalHauteur','TheoreticalHauteurCV']\n",
        "    features_q_5=['PositionofBreakTip','PositionofBreakMiddle','PositionofBreakBase']\n",
        "    features_q=features_q_1+features_q_2+features_q_3+features_q_4+features_q_5\n",
        "    df_q=df.loc[:,features_q]#create view\n",
        "    df_q=standardColumns(df_q,features_q)#apply standardization technique\n",
        "    #see_correlation(df_q,features_q,'SalePrice',df_target)\n",
        "    #check_for_nulls(df_q)\n",
        "\n",
        "\n",
        "    #Filter variables(sold or not sold?):\n",
        "    features_f=['SaleOutcome']\n",
        "    df_f=df.loc[:,features_f]\n",
        "\n",
        "\n",
        "    #features you want\n",
        "    features=features_w+features_d+features_c+features_q+target#you want these features (names only)\n",
        "\n",
        "    #combine columns\n",
        "    data=[df_w,df_d,df_c,df_q,df_target]#data = [df1[\"A\"], df2[\"A\"]]\n",
        "    df_featuere_set = pd.concat(data, axis=1)\n",
        "    print(df_featuere_set.head(5))\n",
        "\n",
        "    #filter\n",
        "    \n",
        "    data=[df_w,df_d,df_c,df_q,df_f,df_target]#make data again with filter variable\n",
        "    df_temp = pd.concat(data, axis=1)#, keys=headers)\n",
        "    print(df_temp.columns)\n",
        "    df_temp= df_temp[df_temp['SaleOutcome'] == 0] \n",
        "    print(df_temp.head(5))\n",
        "    df_featuere_set = df_temp.loc[:,features]#dataframe of features without the filter variable\n",
        "    #'''\n",
        "\n",
        "    print(df_featuere_set.head(5))\n",
        "    return df_featuere_set, features\n",
        "\n",
        "#feature set 4\n",
        "\n",
        "\n",
        "\n",
        "df_feature_set_4, feature_names=select_engg_features(df_big,'train')\n",
        "with open('f4_label_impact_5_10.pickle', 'wb') as f:\n",
        "  pickle.dump(df_feature_set_4, feature_names, f)\n",
        " \n",
        "\n",
        "#df_feature_set_4_last_50, feature_names=select_engg_features(df_big.loc[-50:],'test')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cols to scale ['NetWeight']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Impact coding for SaleNbrSellingCntr\n",
            "inner 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "inner 2\n",
            "inner 3\n",
            "inner 4\n",
            "inner 5\n",
            "inner 6\n",
            "inner 7\n",
            "inner 8\n",
            "inner 9\n",
            "inner 10\n",
            "Impact coding for Brand\n",
            "inner 1\n",
            "inner 2\n",
            "inner 3\n",
            "inner 4\n",
            "inner 5\n",
            "inner 6\n",
            "inner 7\n",
            "inner 8\n",
            "inner 9\n",
            "inner 10\n",
            "Impact coding for StandardBaleDesc\n",
            "inner 1\n",
            "inner 2\n",
            "inner 3\n",
            "inner 4\n",
            "inner 5\n",
            "inner 6\n",
            "inner 7\n",
            "inner 8\n",
            "inner 9\n",
            "inner 10\n",
            "Impact coding for Region\n",
            "inner 1\n",
            "inner 2\n",
            "inner 3\n",
            "inner 4\n",
            "inner 5\n",
            "inner 6\n",
            "inner 7\n",
            "inner 8\n",
            "inner 9\n",
            "inner 10\n",
            "Impact coding for Division\n",
            "inner 1\n",
            "inner 2\n",
            "inner 3\n",
            "inner 4\n",
            "inner 5\n",
            "inner 6\n",
            "inner 7\n",
            "inner 8\n",
            "inner 9\n",
            "inner 10\n",
            "Impact coding for Branch\n",
            "inner 1\n",
            "inner 2\n",
            "inner 3\n",
            "inner 4\n",
            "inner 5\n",
            "inner 6\n",
            "inner 7\n",
            "inner 8\n",
            "inner 9\n",
            "inner 10\n",
            "Impact coding for WAM\n",
            "inner 1\n",
            "inner 2\n",
            "inner 3\n",
            "inner 4\n",
            "inner 5\n",
            "inner 6\n",
            "inner 7\n",
            "inner 8\n",
            "inner 9\n",
            "inner 10\n",
            "Impact coding for Agent\n",
            "inner 1\n",
            "inner 2\n",
            "inner 3\n",
            "inner 4\n",
            "inner 5\n",
            "inner 6\n",
            "inner 7\n",
            "inner 8\n",
            "inner 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrMkzZ695Uvc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "cdaa72df-4232-440b-afd1-6af5dcce88e1"
      },
      "source": [
        "print(df_feature_set_4.loc[-50:].head(5))\n",
        "print(df_feature_set_4_last_50.head(5))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   NetWeight  Month  ...  PositionofBreakBase  SalePrice\n",
            "0   0.101094      1  ...             1.652904     1250.0\n",
            "1   0.065628      1  ...             0.418474     1198.0\n",
            "2   0.086178      1  ...            -0.724517     1208.0\n",
            "3   0.070103      1  ...            -0.724517     1208.0\n",
            "4   0.047232      1  ...             0.281315     1411.0\n",
            "\n",
            "[5 rows x 28 columns]\n",
            "   NetWeight  Month  ...  PositionofBreakBase  SalePrice\n",
            "0   0.101094      1  ...             1.652904     1250.0\n",
            "1   0.065628      1  ...             0.418474     1198.0\n",
            "2   0.086178      1  ...            -0.724517     1208.0\n",
            "3   0.070103      1  ...            -0.724517     1208.0\n",
            "4   0.047232      1  ...             0.281315     1411.0\n",
            "\n",
            "[5 rows x 28 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGaFn2TP2HAY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a3effb48-e7f1-468d-9d29-e9e8681431c5"
      },
      "source": [
        "df_feature_set_4_last_50, feature_names=select_engg_features(df_big.loc[-50:],'test')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cols to scale ['NetWeight']\n",
            "label on test data\n",
            "Index(['Month'], dtype='object')\n",
            "Index(['Month'], dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "label on test data\n",
            "Index(['SaleNbrSellingCntr', 'Brand', 'StandardBaleDesc', 'Region', 'Division',\n",
            "       'Branch', 'WAM', 'Agent'],\n",
            "      dtype='object')\n",
            "Index(['SaleNbrSellingCntr', 'Brand', 'StandardBaleDesc', 'Region', 'Division',\n",
            "       'Branch', 'WAM', 'Agent'],\n",
            "      dtype='object')\n",
            "SaleNbrSellingCntr here SaleNbrSellingCntr\n",
            "0    973.418888\n",
            "1    949.767007\n",
            "2    982.615005\n",
            "dtype: float64 833.1187716340875\n",
            "Brand here Brand\n",
            "0        1137.866667\n",
            "1         861.683333\n",
            "3         817.500000\n",
            "4        1004.437500\n",
            "5        1160.500000\n",
            "            ...     \n",
            "16053     774.650000\n",
            "16054     867.453682\n",
            "16055     739.093205\n",
            "16056     900.453682\n",
            "16057     755.979167\n",
            "Length: 11245, dtype: float64 833.1187716340875\n",
            "StandardBaleDesc here StandardBaleDesc\n",
            "0        766.059385\n",
            "1       1036.559385\n",
            "2       1098.000000\n",
            "3       1160.559385\n",
            "4       1051.559385\n",
            "           ...     \n",
            "6235    1554.291667\n",
            "6237    1653.866667\n",
            "6238    1602.166667\n",
            "6239     534.059385\n",
            "6241     281.630729\n",
            "Length: 3820, dtype: float64 833.1187716340875\n",
            "Region here Region\n",
            "0    1026.283737\n",
            "1     955.009795\n",
            "2     960.811443\n",
            "3     962.724043\n",
            "dtype: float64 833.1187716340875\n",
            "Division here Division\n",
            "0    1041.935488\n",
            "1     800.975298\n",
            "2     898.056787\n",
            "3     960.042017\n",
            "4     926.341641\n",
            "5     997.181345\n",
            "6     937.104791\n",
            "7     974.703772\n",
            "8     962.722857\n",
            "dtype: float64 833.1187716340875\n",
            "Branch here Branch\n",
            "0      1047.700308\n",
            "1       962.600643\n",
            "2       835.214912\n",
            "3       664.911706\n",
            "4      1094.540273\n",
            "          ...     \n",
            "239     937.243945\n",
            "240     564.335064\n",
            "241     950.520652\n",
            "242     958.225122\n",
            "243     962.722412\n",
            "Length: 224, dtype: float64 833.1187716340875\n",
            "WAM here WAM\n",
            "0     1011.852300\n",
            "2      901.108164\n",
            "3      993.879240\n",
            "4     1306.036507\n",
            "6     1115.756046\n",
            "         ...     \n",
            "92    1000.670549\n",
            "93     958.776599\n",
            "94     899.125000\n",
            "95     962.721676\n",
            "96     818.783084\n",
            "Length: 84, dtype: float64 833.1187716340875\n",
            "Agent here Agent\n",
            "0       976.059386\n",
            "1      1128.007305\n",
            "2      1124.666667\n",
            "3       991.559386\n",
            "4      1020.285810\n",
            "          ...     \n",
            "648     735.726053\n",
            "649    1051.041667\n",
            "650    1173.437500\n",
            "651    1158.472804\n",
            "652     935.798024\n",
            "Length: 611, dtype: float64 833.1187716340875\n",
            "   NetWeight  Month  ...  PositionofBreakBase  SalePrice\n",
            "0   0.101094      1  ...             1.652904     1250.0\n",
            "1   0.065628      1  ...             0.418474     1198.0\n",
            "2   0.086178      1  ...            -0.724517     1208.0\n",
            "3   0.070103      1  ...            -0.724517     1208.0\n",
            "4   0.047232      1  ...             0.281315     1411.0\n",
            "\n",
            "[5 rows x 28 columns]\n",
            "   NetWeight  Month  ...  PositionofBreakBase  SalePrice\n",
            "0   0.101094      1  ...             1.652904     1250.0\n",
            "1   0.065628      1  ...             0.418474     1198.0\n",
            "2   0.086178      1  ...            -0.724517     1208.0\n",
            "3   0.070103      1  ...            -0.724517     1208.0\n",
            "4   0.047232      1  ...             0.281315     1411.0\n",
            "\n",
            "[5 rows x 28 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ruJQHCL8xuu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "8ab8ec8d-c20c-4bee-a0fe-49ff8c4c9f67"
      },
      "source": [
        "    #Train set experiment on a new feature engg\n",
        "    label=['SalePrice']\n",
        "    target=['SalePrice']\n",
        "    df_target=df.loc[:,target]#create view\n",
        "    \n",
        "    \n",
        "    #seller variables: train\n",
        "    features_c_1=['SaleNbrSellingCntr', 'Brand','StandardBaleDesc']\n",
        "    features_c_2=['Region','Division','Branch','WAM','Agent']\n",
        "    features_c=features_c_1+features_c_2\n",
        "    df_c=df.loc[:,features_c]#create view\n",
        "    df_c=df_c.fillna('unknown')#replace missing values\n",
        "    \n",
        "\n",
        "    df_c=apply_label_encoder(df_c,features_c,'train')#apply label encoder\n",
        "    #see_correlation(df_c,features_c,'SalePrice',df_target)\n",
        "    #df_c=meanEncoding(df_c, features_c,'SalePrice',df_target,'train')#apply mean encoder after label encoder, mean encoding displays correlation also\n",
        "    df_c=apply_impact_encoder(df_c, features_c,'SalePrice',df_target,'train')#apply mean encoder after label encoder, mean encoding displays correlation also\n",
        "    #check_for_nulls(df_c)\n",
        "    print(df_c.head(5))\n",
        "    \n",
        "\n",
        "    \n",
        "    #Test set experiment on a new feature engg\n",
        "    df_c_test= df.loc[0:100]#[-6:] #last 6 rows from raw data\n",
        "    df_c_test=df_c_test.loc[:,features_c]#create view\n",
        "    print(df_c_test.head(5))\n",
        "    df_c_test=df_c_test.fillna('unknown')#replace missing values\n",
        "    df_c_test=apply_label_encoder(df_c_test,features_c,'test')#apply label encoder\n",
        "    #see_correlation(df_c,features_c,'SalePrice',df_target)\n",
        "    #df_c_test=meanEncoding(df_c_test, features_c,'SalePrice',df_target,'test')#apply mean encoder after label encoder, mean encoding displays correlation also\n",
        "    df_c_test=apply_impact_encoder(df_c_test, features_c,'SalePrice',df_target,'test')#apply mean encoder after label encoder, mean encoding displays correlation also\n",
        "    #check_for_nulls(df_c)\n",
        "    print(df_c_test.head(5))\n",
        "\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Impact coding for SaleNbrSellingCntr\n",
            "inner\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "inner\n",
            "Impact coding for Brand\n",
            "inner\n",
            "inner\n",
            "Impact coding for StandardBaleDesc\n",
            "inner\n",
            "inner\n",
            "Impact coding for Region\n",
            "inner\n",
            "inner\n",
            "Impact coding for Division\n",
            "inner\n",
            "inner\n",
            "Impact coding for Branch\n",
            "inner\n",
            "inner\n",
            "Impact coding for WAM\n",
            "inner\n",
            "inner\n",
            "Impact coding for Agent\n",
            "inner\n",
            "inner\n",
            "   SaleNbrSellingCntr        Brand  ...          WAM        Agent\n",
            "0          982.614976  1312.352941  ...  1171.361615  1182.706972\n",
            "1          982.615033  1296.334028  ...  1171.361615  1182.706972\n",
            "2          982.615033  1312.352941  ...  1171.420970  1178.224638\n",
            "3          982.615033  1312.352941  ...  1171.361615  1178.224638\n",
            "4          982.614976  1296.334028  ...  1171.361615  1182.706972\n",
            "\n",
            "[5 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIFFFvJU6QYP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bab3dd7e-dc0f-4f01-f321-616ca24ac10e"
      },
      "source": [
        "#Test set experiment on a new feature engg\n",
        "df_c_test= df.loc[-6:] #last 6 rows from raw data\n",
        "df_c_test=df_c_test.loc[:,features_c]#create view\n",
        "print(df_c_test.head(5))\n",
        "df_c_test=df_c_test.fillna('unknown')#replace missing values\n",
        "df_c_test=apply_label_encoder(df_c_test,features_c,'test')#apply label encoder\n",
        "print(df_c_test.head(5))\n",
        "#see_correlation(df_c,features_c,'SalePrice',df_target)\n",
        "#df_c_test=meanEncoding(df_c_test, features_c,'SalePrice',df_target,'test')#apply mean encoder after label encoder, mean encoding displays correlation also\n",
        "df_c_test=apply_impact_encoder(df_c_test, features_c,'SalePrice',df_target,'test')#apply mean encoder after label encoder, mean encoding displays correlation also\n",
        "#check_for_nulls(df_c)\n",
        "print(df_c_test.head(5))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  SaleNbrSellingCntr      Brand  ...            WAM              Agent\n",
            "0                  S  YARRAFORD  ...  NICKY  SYMONS  BP24-BRAD NEWSOME\n",
            "1                  S  YARRAFORD  ...  NICKY  SYMONS  BP24-BRAD NEWSOME\n",
            "2                  S  YARRAFORD  ...  NICKY  SYMONS  BP24-BRAD NEWSOME\n",
            "3                  S  YARRAFORD  ...  NICKY  SYMONS  BP24-BRAD NEWSOME\n",
            "4                  S  YARRAFORD  ...  NICKY  SYMONS  BP24-BRAD NEWSOME\n",
            "\n",
            "[5 rows x 8 columns]\n",
            "label on test data\n",
            "(632971, 8)\n",
            "   SaleNbrSellingCntr  Brand  StandardBaleDesc  ...  Branch  WAM  Agent\n",
            "0                   2  15920               811  ...      91   62     89\n",
            "1                   2  15920              3989  ...      91   62     89\n",
            "2                   2  15920              1842  ...      91   62     89\n",
            "3                   2  15920               811  ...      91   62     89\n",
            "4                   2  15920               811  ...      91   62     89\n",
            "\n",
            "[5 rows x 8 columns]\n",
            "SaleNbrSellingCntr here SaleNbrSellingCntr\n",
            "0    973.418888\n",
            "1    949.767007\n",
            "2    982.615005\n",
            "dtype: float64 833.1187716340875\n",
            "Brand here Brand\n",
            "0        1137.866667\n",
            "1         861.683333\n",
            "3         817.500000\n",
            "4        1004.437500\n",
            "5        1160.500000\n",
            "            ...     \n",
            "16053     774.650000\n",
            "16054     867.453682\n",
            "16055     739.093205\n",
            "16056     900.453682\n",
            "16057     755.979167\n",
            "Length: 11245, dtype: float64 833.1187716340875\n",
            "StandardBaleDesc here StandardBaleDesc\n",
            "0        766.059385\n",
            "1       1036.559385\n",
            "2       1098.000000\n",
            "3       1160.559385\n",
            "4       1051.559385\n",
            "           ...     \n",
            "6235    1554.291667\n",
            "6237    1653.866667\n",
            "6238    1602.166667\n",
            "6239     534.059385\n",
            "6241     281.630729\n",
            "Length: 3820, dtype: float64 833.1187716340875\n",
            "Region here Region\n",
            "0    1026.283737\n",
            "1     955.009795\n",
            "2     960.811443\n",
            "3     962.724043\n",
            "dtype: float64 833.1187716340875\n",
            "Division here Division\n",
            "0    1041.935488\n",
            "1     800.975298\n",
            "2     898.056787\n",
            "3     960.042017\n",
            "4     926.341641\n",
            "5     997.181345\n",
            "6     937.104791\n",
            "7     974.703772\n",
            "8     962.722857\n",
            "dtype: float64 833.1187716340875\n",
            "Branch here Branch\n",
            "0      1047.700308\n",
            "1       962.600643\n",
            "2       835.214912\n",
            "3       664.911706\n",
            "4      1094.540273\n",
            "          ...     \n",
            "239     937.243945\n",
            "240     564.335064\n",
            "241     950.520652\n",
            "242     958.225122\n",
            "243     962.722412\n",
            "Length: 224, dtype: float64 833.1187716340875\n",
            "WAM here WAM\n",
            "0     1011.852300\n",
            "2      901.108164\n",
            "3      993.879240\n",
            "4     1306.036507\n",
            "6     1115.756046\n",
            "         ...     \n",
            "92    1000.670549\n",
            "93     958.776599\n",
            "94     899.125000\n",
            "95     962.721676\n",
            "96     818.783084\n",
            "Length: 84, dtype: float64 833.1187716340875\n",
            "Agent here Agent\n",
            "0       976.059386\n",
            "1      1128.007305\n",
            "2      1124.666667\n",
            "3       991.559386\n",
            "4      1020.285810\n",
            "          ...     \n",
            "648     735.726053\n",
            "649    1051.041667\n",
            "650    1173.437500\n",
            "651    1158.472804\n",
            "652     935.798024\n",
            "Length: 611, dtype: float64 833.1187716340875\n",
            "   SaleNbrSellingCntr        Brand  ...          WAM        Agent\n",
            "0          982.615005  1304.343484  ...  1171.391292  1180.465805\n",
            "1          982.615005  1304.343484  ...  1171.391292  1180.465805\n",
            "2          982.615005  1304.343484  ...  1171.391292  1180.465805\n",
            "3          982.615005  1304.343484  ...  1171.391292  1180.465805\n",
            "4          982.615005  1304.343484  ...  1171.391292  1180.465805\n",
            "\n",
            "[5 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6UzY8g84UoA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "161e63a3-280b-4382-88c1-5f9a9c37f59d"
      },
      "source": [
        "a=5\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yoWjJlyeGlp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "outputId": "55772136-a28f-44cd-da5f-da9ae1b9fb05"
      },
      "source": [
        "#feature set 3 (no mean encoding without not-sold wool, not-sold wool are filtered using filter variable)\n",
        "def select_engg_features(df_test):#update this function based on selected feature, encoding you want to apply and filter technique\n",
        "    df=df_test\n",
        "    #target variable:\n",
        "    label=['SalePrice']\n",
        "    target=['SalePrice']\n",
        "    df_target=df.loc[:,target]#create view\n",
        "\n",
        "\n",
        "    #weight variables:\n",
        "    features_w=['NetWeight']#from all the weights only using net weight since others are correlated\n",
        "    df_w=df.loc[:,features_w]#create view\n",
        "    df_w=scaleColumns(df_w,features_w)#apply normalization\n",
        "    #see_correlation(df_w,features_w,'SalePrice',df_target)\n",
        "    #check_for_nulls(df_w)\n",
        "\n",
        "\n",
        "    #date variables:\n",
        "    features_d=['SaleDate']#SaleDate will change to 'Month'\n",
        "    df_date=df.loc[:,features_d]#create view\n",
        "    df_date= extract_year_month_day(df_date,'month')#extract month column, this will be added to the dataframe as a new column 'Month'\n",
        "    df_d=label_encoder(df_date[['Month']],['Month'])#apply label encoder: deal month as categorical or the model would give the most importance to 12\n",
        "    features_d=['Month']\n",
        "    #see_correlation(df_d,features_d,'SalePrice',df_target)\n",
        "    #check_for_nulls(df_d)\n",
        "\n",
        "    #seller variables:\n",
        "    features_c_1=['SaleNbrSellingCntr', 'Brand','StandardBaleDesc']\n",
        "    features_c_2=['Region','Division','Branch','WAM','Agent']\n",
        "    features_c=features_c_1+features_c_2\n",
        "    df_c=df.loc[:,features_c]#create view\n",
        "    df_c.fillna('unknown')#replace missing values\n",
        "    df_c=apply_label_encoder(df_c,features_c)#apply label encoder\n",
        "    see_correlation(df_c,features_c,'SalePrice',df_target)\n",
        "    #df_c=meanEncoding(df_c, features_c,'SalePrice',df_target)#apply mean encoder after label encoder, mean encoding displays correlation also\n",
        "    #check_for_nulls(df_c)\n",
        "\n",
        "\n",
        "\n",
        "    #quality variables: \n",
        "    features_q_1 = ['Micron','Curvature','VMB','Yield1','FolioProceeds']\n",
        "    features_q_2=['FDMean','FDCOEfficient','FDComfort']\n",
        "    features_q_3=['StapleLength','StapleLengthCV','StapleStrength','StapleStrengthLowest25']\n",
        "    features_q_4=['TheoreticalHauteur','TheoreticalHauteurCV']\n",
        "    features_q_5=['PositionofBreakTip','PositionofBreakMiddle','PositionofBreakBase']\n",
        "    features_q=features_q_1+features_q_2+features_q_3+features_q_4+features_q_5\n",
        "    df_q=df.loc[:,features_q]#create view\n",
        "    df_q=standardColumns(df_q,features_q)#apply standardization technique\n",
        "    #see_correlation(df_q,features_q,'SalePrice',df_target)\n",
        "    #check_for_nulls(df_q)\n",
        "\n",
        "\n",
        "    #Filter variables(sold or not sold?):\n",
        "    features_f=['SaleOutcome']\n",
        "    df_f=df.loc[:,features_f]\n",
        "\n",
        "\n",
        "    #features you want\n",
        "    features=features_w+features_d+features_c+features_q+target#you want these features (names only)\n",
        "\n",
        "    #combine columns\n",
        "    data=[df_w,df_d,df_c,df_q,df_target]#data = [df1[\"A\"], df2[\"A\"]]\n",
        "    df_featuere_set = pd.concat(data, axis=1)\n",
        "    print(df_featuere_set.head(5))\n",
        "\n",
        "    #filter\n",
        "    data=[df_w,df_d,df_c,df_q,df_f,df_target]#make data again with filter variable\n",
        "    df_temp = pd.concat(data, axis=1)#, keys=headers)\n",
        "    print(df_temp.columns)\n",
        "    df_temp= df_temp[df_temp['SaleOutcome'] == 0] \n",
        "    print(df_temp.head(5))\n",
        "    df_featuere_set = df_temp.loc[:,features]#dataframe of features without the filter variable\n",
        "\n",
        "\n",
        "    print(df_featuere_set.head(5))\n",
        "    return df_featuere_set, features\n",
        "\n",
        "#feature set 3\n",
        "df_feature_set_3, feature_names=select_engg_features(df_big) \n",
        "df_feature_set_3_train=df_feature_set_3[0:-6]#train data without last 6 data\n",
        "df_feature_set_3_test=df_feature_set_3[-6:]#last 6 test data FE applied\n",
        "print(df_feature_set_3.shape)\n",
        "print(df_feature_set_3_train.shape)\n",
        "print(df_feature_set_3_test.shape)\n",
        "\n",
        "#df_feature_set_3_train.to_csv('woolCaret_feature_set_3.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cols to scale ['NetWeight']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "helo see_correlation\n",
            "SaleNbrSellingCntr  correlation  0.008269741504885807\n",
            "Brand  correlation  0.00328249202058887\n",
            "StandardBaleDesc  correlation  -0.15212894381654213\n",
            "Region  correlation  0.035962264738053064\n",
            "Division  correlation  0.03682637568384327\n",
            "Branch  correlation  0.01364686189042153\n",
            "WAM  correlation  0.023953403099308657\n",
            "Agent  correlation  -0.03545020848048214\n",
            "   NetWeight  Month  ...  PositionofBreakBase  SalePrice\n",
            "0   0.101094      1  ...             1.652904     1250.0\n",
            "1   0.065628      1  ...             0.418474     1198.0\n",
            "2   0.086178      1  ...            -0.724517     1208.0\n",
            "3   0.070103      1  ...            -0.724517     1208.0\n",
            "4   0.047232      1  ...             0.281315     1411.0\n",
            "\n",
            "[5 rows x 28 columns]\n",
            "Index(['NetWeight', 'Month', 'SaleNbrSellingCntr', 'Brand', 'StandardBaleDesc',\n",
            "       'Region', 'Division', 'Branch', 'WAM', 'Agent', 'Micron', 'Curvature',\n",
            "       'VMB', 'Yield1', 'FolioProceeds', 'FDMean', 'FDCOEfficient',\n",
            "       'FDComfort', 'StapleLength', 'StapleLengthCV', 'StapleStrength',\n",
            "       'StapleStrengthLowest25', 'TheoreticalHauteur', 'TheoreticalHauteurCV',\n",
            "       'PositionofBreakTip', 'PositionofBreakMiddle', 'PositionofBreakBase',\n",
            "       'SaleOutcome', 'SalePrice'],\n",
            "      dtype='object')\n",
            "   NetWeight  Month  ...  SaleOutcome  SalePrice\n",
            "0   0.101094      1  ...            0     1250.0\n",
            "1   0.065628      1  ...            0     1198.0\n",
            "2   0.086178      1  ...            0     1208.0\n",
            "3   0.070103      1  ...            0     1208.0\n",
            "4   0.047232      1  ...            0     1411.0\n",
            "\n",
            "[5 rows x 29 columns]\n",
            "   NetWeight  Month  ...  PositionofBreakBase  SalePrice\n",
            "0   0.101094      1  ...             1.652904     1250.0\n",
            "1   0.065628      1  ...             0.418474     1198.0\n",
            "2   0.086178      1  ...            -0.724517     1208.0\n",
            "3   0.070103      1  ...            -0.724517     1208.0\n",
            "4   0.047232      1  ...             0.281315     1411.0\n",
            "\n",
            "[5 rows x 28 columns]\n",
            "(595615, 28)\n",
            "(595609, 28)\n",
            "(6, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkBXM9y_5QCa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "040ec37d-7121-44f7-bada-b81c5451e900"
      },
      "source": [
        "#feature set 2 (with mean encoding and without not-sold wool, not sold wool are filtered)\n",
        "def select_engg_features(df_test):#update this function based on selected feature, encoding you want to apply and filter technique\n",
        "    df=df_test\n",
        "    #target variable:\n",
        "    label=['SalePrice']\n",
        "    target=['SalePrice']\n",
        "    df_target=df.loc[:,target]#create view\n",
        "\n",
        "\n",
        "    #weight variables:\n",
        "    features_w=['NetWeight']#from all the weights only using net weight since others are correlated\n",
        "    df_w=df.loc[:,features_w]#create view\n",
        "    df_w=scaleColumns(df_w,features_w)#apply normalization\n",
        "    #see_correlation(df_w,features_w,'SalePrice',df_target)\n",
        "    #check_for_nulls(df_w)\n",
        "\n",
        "\n",
        "    #date variables:\n",
        "    features_d=['SaleDate']#SaleDate will change to 'Month'\n",
        "    df_date=df.loc[:,features_d]#create view\n",
        "    df_date= extract_year_month_day(df_date,'month')#extract month column, this will be added to the dataframe as a new column 'Month'\n",
        "    df_d=label_encoder(df_date[['Month']],['Month'])#apply label encoder: deal month as categorical or the model would give the most importance to 12\n",
        "    features_d=['Month']\n",
        "    #see_correlation(df_d,features_d,'SalePrice',df_target)\n",
        "    #check_for_nulls(df_d)\n",
        "\n",
        "    #seller variables:\n",
        "    features_c_1=['SaleNbrSellingCntr', 'Brand','StandardBaleDesc']\n",
        "    features_c_2=['Region','Division','Branch','WAM','Agent']\n",
        "    features_c=features_c_1+features_c_2\n",
        "    df_c=df.loc[:,features_c]#create view\n",
        "    df_c.fillna('unknown')#replace missing values\n",
        "    df_c=apply_label_encoder(df_c,features_c)#apply label encoder\n",
        "    see_correlation(df_c,features_c,'SalePrice',df_target)\n",
        "    df_c=meanEncoding(df_c, features_c,'SalePrice',df_target)#apply mean encoder after label encoder, mean encoding displays correlation also\n",
        "    #check_for_nulls(df_c)\n",
        "\n",
        "\n",
        "\n",
        "    #quality variables: \n",
        "    features_q_1 = ['Micron','Curvature','VMB','Yield1','FolioProceeds']\n",
        "    features_q_2=['FDMean','FDCOEfficient','FDComfort']\n",
        "    features_q_3=['StapleLength','StapleLengthCV','StapleStrength','StapleStrengthLowest25']\n",
        "    features_q_4=['TheoreticalHauteur','TheoreticalHauteurCV']\n",
        "    features_q_5=['PositionofBreakTip','PositionofBreakMiddle','PositionofBreakBase']\n",
        "    features_q=features_q_1+features_q_2+features_q_3+features_q_4+features_q_5\n",
        "    df_q=df.loc[:,features_q]#create view\n",
        "    df_q=standardColumns(df_q,features_q)#apply standardization technique\n",
        "    #see_correlation(df_q,features_q,'SalePrice',df_target)\n",
        "    #check_for_nulls(df_q)\n",
        "\n",
        "\n",
        "    #Filter variables(sold or not sold?):\n",
        "    features_f=['SaleOutcome']\n",
        "    df_f=df.loc[:,features_f]\n",
        "\n",
        "\n",
        "    #features you want\n",
        "    features=features_w+features_d+features_c+features_q+target#you want these features (names only)\n",
        "\n",
        "    #combine columns\n",
        "    data=[df_w,df_d,df_c,df_q,df_target]#data = [df1[\"A\"], df2[\"A\"]]\n",
        "    df_featuere_set = pd.concat(data, axis=1)\n",
        "    print(df_featuere_set.head(5))\n",
        "\n",
        "    #filter\n",
        "    data=[df_w,df_d,df_c,df_q,df_f,df_target]#make data again with filter variable\n",
        "    df_temp = pd.concat(data, axis=1)#, keys=headers)\n",
        "    print(df_temp.columns)\n",
        "    df_temp= df_temp[df_temp['SaleOutcome'] == 0] \n",
        "    print(df_temp.head(5))\n",
        "    df_featuere_set = df_temp.loc[:,features]#dataframe of features without the filter variable\n",
        "\n",
        "\n",
        "    print(df_featuere_set.head(5))\n",
        "    return df_featuere_set, features\n",
        "\n",
        "#feature set 2\n",
        "df_feature_set_2, feature_names=select_engg_features(df_big) \n",
        "df_feature_set_2_train=df_feature_set_2[0:-6]#train data without last 6 data\n",
        "df_feature_set_2_test=df_feature_set_2[-6:]#last 6 test data FE applied\n",
        "print(df_feature_set_2.shape)\n",
        "print(df_feature_set_2_train.shape)\n",
        "print(df_feature_set_2_test.shape)\n",
        "\n",
        "#df_feature_set_2_train.to_csv('woolCaret_feature_set_2.csv')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cols to scale ['NetWeight']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "helo see_correlation\n",
            "SaleNbrSellingCntr  correlation  0.008269741504885807\n",
            "Brand  correlation  0.00328249202058887\n",
            "StandardBaleDesc  correlation  -0.15212894381654213\n",
            "Region  correlation  0.035962264738053064\n",
            "Division  correlation  0.03682637568384327\n",
            "Branch  correlation  0.01364686189042153\n",
            "WAM  correlation  0.023953403099308657\n",
            "Agent  correlation  -0.03545020848048214\n",
            "SaleNbrSellingCntr 0.011214492663550743\n",
            "Brand 0.46350765706845665\n",
            "StandardBaleDesc 0.6893155354626512\n",
            "Region 0.061001556311784506\n",
            "Division 0.07996289729990345\n",
            "Branch 0.15462966344792534\n",
            "WAM 0.15062311662308325\n",
            "Agent 0.1949854022573914\n",
            "   NetWeight  Month  ...  PositionofBreakBase  SalePrice\n",
            "0   0.101094      1  ...             1.652904     1250.0\n",
            "1   0.065628      1  ...             0.418474     1198.0\n",
            "2   0.086178      1  ...            -0.724517     1208.0\n",
            "3   0.070103      1  ...            -0.724517     1208.0\n",
            "4   0.047232      1  ...             0.281315     1411.0\n",
            "\n",
            "[5 rows x 28 columns]\n",
            "Index(['NetWeight', 'Month', 'SaleNbrSellingCntr', 'Brand', 'StandardBaleDesc',\n",
            "       'Region', 'Division', 'Branch', 'WAM', 'Agent', 'Micron', 'Curvature',\n",
            "       'VMB', 'Yield1', 'FolioProceeds', 'FDMean', 'FDCOEfficient',\n",
            "       'FDComfort', 'StapleLength', 'StapleLengthCV', 'StapleStrength',\n",
            "       'StapleStrengthLowest25', 'TheoreticalHauteur', 'TheoreticalHauteurCV',\n",
            "       'PositionofBreakTip', 'PositionofBreakMiddle', 'PositionofBreakBase',\n",
            "       'SaleOutcome', 'SalePrice'],\n",
            "      dtype='object')\n",
            "   NetWeight  Month  ...  SaleOutcome  SalePrice\n",
            "0   0.101094      1  ...            0     1250.0\n",
            "1   0.065628      1  ...            0     1198.0\n",
            "2   0.086178      1  ...            0     1208.0\n",
            "3   0.070103      1  ...            0     1208.0\n",
            "4   0.047232      1  ...            0     1411.0\n",
            "\n",
            "[5 rows x 29 columns]\n",
            "   NetWeight  Month  ...  PositionofBreakBase  SalePrice\n",
            "0   0.101094      1  ...             1.652904     1250.0\n",
            "1   0.065628      1  ...             0.418474     1198.0\n",
            "2   0.086178      1  ...            -0.724517     1208.0\n",
            "3   0.070103      1  ...            -0.724517     1208.0\n",
            "4   0.047232      1  ...             0.281315     1411.0\n",
            "\n",
            "[5 rows x 28 columns]\n",
            "(595615, 28)\n",
            "(595609, 28)\n",
            "(6, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY8ZYB5goLb8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c10c97ba-848e-43e5-da6b-98d4bf912eb0"
      },
      "source": [
        "feature_names\n",
        "features=feature_names[0:-1]\n",
        "target=feature_names[-1:]\n",
        "print(feature_names)\n",
        "print(target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['NetWeight', 'Month', 'SaleNbrSellingCntr', 'Brand', 'StandardBaleDesc', 'Region', 'Division', 'Branch', 'WAM', 'Agent', 'Micron', 'Curvature', 'VMB', 'Yield1', 'FolioProceeds', 'FDMean', 'FDCOEfficient', 'FDComfort', 'StapleLength', 'StapleLengthCV', 'StapleStrength', 'StapleStrengthLowest25', 'TheoreticalHauteur', 'TheoreticalHauteurCV', 'PositionofBreakTip', 'PositionofBreakMiddle', 'PositionofBreakBase', 'SalePrice']\n",
            "['SalePrice']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}